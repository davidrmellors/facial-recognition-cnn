# Facial Recognition CNN - PDAN8412 Portfolio of Evidence

## Project Overview

This project implements a deep learning facial recognition system using Convolutional Neural Networks (CNNs) for the PDAN8412 Portfolio of Evidence. The system classifies 150 unique identities from synthetic face images with high accuracy (97.6% on test set, 97.43% validation).

## Dataset

**Source**: Microsoft DigiFace-1M  
**Subset Used**: 150 identities Ã— 72 images per identity = **10,800 images**

### Dataset Characteristics
- **Image Dimensions**: 112Ã—112 pixels (RGB with alpha channel)
- **Format**: PNG files
- **Licensing**: Non-commercial research use (academic assessment compliant)
- **Quality**: Photorealistic synthetic faces with pose, lighting, and accessory variations
- **Split**: 70% training (7,513), 20% validation (2,216), 10% test (1,071)

### Why Synthetic Data?
- Avoids privacy concerns associated with real facial data
- Maintains high quality and consistency
- Captures real-world variation (pose, expression, accessories)
- Suitable for academic research requirements

## Technical Architecture

### Tools & Technologies
- **Apache Spark**: Large-scale data ingestion, EDA, and preprocessing
- **TensorFlow/Keras**: Deep learning framework
- **Python 3.12**: Core programming language
- **Additional Libraries**: NumPy, Matplotlib, scikit-learn, PIL, KerasTuner

### Data Processing Pipeline

1. **Spark-Powered Ingestion**
   - Loaded 10,800 images using binary file format
   - Extracted labels from directory structure
   - Verified data integrity (zero null payloads)

2. **Exploratory Data Analysis**
   - **Class Balance**: Perfectly balanced (72 images per identity)
   - **Dimension Uniformity**: All images are 112Ã—112Ã—4
   - **Quality Checks**: No corrupted or invalid files

3. **Feature Engineering**
   - Retained RGB color channels (channel variance analysis justified color over grayscale)
   - Pixel value rescaling (1/255 normalization)
   - Data augmentation strategy:
     - Horizontal flips
     - Random rotations (Â±10%)
     - Random zoom (Â±20%)
     - Random translations (Â±20%)

## Model Architecture

### CNN Design
**Architecture Type**: Residual CNN with Separable Convolutions

**Key Components**:
- **Input**: 112Ã—112Ã—3 RGB images
- **Initial Block**: Conv2D (128 filters) + BatchNorm + ReLU
- **Residual Blocks**: 3 blocks with [256, 512, 728] filters
  - SeparableConv2D layers (efficient parameter usage)
  - Batch Normalization
  - MaxPooling with stride 2
  - Residual connections for gradient flow
- **Final Layers**:
  - SeparableConv2D (1024 filters)
  - GlobalAveragePooling2D
  - Dropout (0.25 - empirically validated as optimal)
  - Dense output layer (150 classes, no activation for logits)

**Total Parameters**: Optimized using depthwise separable convolutions

### Training Configuration

**Base Model Training**:
- **Optimizer**: Adam with Cosine Decay learning rate schedule
  - Initial LR: 1e-3
  - Final LR: 1e-5 (alpha=1e-2)
- **Loss Function**: Sparse Categorical Crossentropy (from logits)
- **Batch Size**: 128
- **Epochs**: 50 (with early stopping)
- **Callbacks**:
  - ModelCheckpoint (save best validation accuracy)
  - EarlyStopping (patience=6, monitor val_acc)

**Hyperparameter Tuning**:
- Manual variant sweep across dropout rates and learning rates
- KerasTuner Hyperband search (30 trials, 53.5 minutes)
- **Optimal configuration validated**: dropout=0.25, learning_rate=1e-3
- **Key finding**: KerasTuner independently discovered the exact same hyperparameters as the original model

## Results

### Performance Metrics

| Metric | Training | Validation | Test |
|--------|----------|------------|------|
| **Accuracy** | **100.0%** | **97.43%** | **97.6%** |
| **Loss** | 0.004 | 0.096 | N/A |

### Best Model Performance
- **Best Validation Epoch**: 48/50
- **Training Accuracy**: 100.0% (perfect training fit)
- **Validation Accuracy**: 97.43%
- **Test Accuracy**: 97.6% (1,045/1,071 correct predictions)
- **Generalization**: Exceptional - only 2.4% train-val gap despite perfect training fit
- **Training Time**: ~15-16 minutes for 50 epochs (~18s per epoch)

### Classification Insights
- **Perfect Classes**: 107 out of 150 classes (71%) achieve perfect 1.000 F1-score
- **Near-Perfect**: 145 out of 150 classes (97%) achieve F1 â‰¥ 0.90
- **Macro-averaged Metrics**: Precision: 97.7%, Recall: 97.4%, F1: 97.3%
- **Weighted-averaged Metrics**: Precision: 97.9%, Recall: 97.6%, F1: 97.6%
- **Edge Cases**: Few classes show lower performance (e.g., class 149: 0.667 F1-score due to small test sample size)
- **Misclassification Causes**: Primarily affect identities with n<5 test samples or visually similar facial structures
- **Total Errors**: Only 26 misclassifications out of 1,071 test predictions (2.4% error rate)

### Sample Predictions
Visual inspection of 6 random test images showed **0 misclassifications**, including:
- Faces with heavy occlusion (respirator mask covering ~40% of face)
- Multiple images of same identity with different accessories
- Profile vs. frontal views
- Varying lighting conditions

## Key Achievements

### Model Performance
1. ðŸ† **97.6% Test Accuracy**: 1,045 out of 1,071 correct predictions (only 26 errors)
2. ðŸ† **97.43% Validation Accuracy**: Achieved at epoch 48 with stable convergence
3. ðŸ† **100% Training Accuracy**: Perfect fit with minimal overfitting (2.4% train-val gap)
4. ðŸ† **71% Perfect Classes**: 107 out of 150 identities classified with perfect F1=1.000
5. ðŸ† **Production-Ready**: Handles occlusions, accessories, lighting, and pose variations

### Hyperparameter Validation
1. âœ… **Empirically Validated**: Three independent experiments confirmed dropout=0.25, lr=1e-3
2. âœ… **Automated Confirmation**: KerasTuner independently discovered the same optimal hyperparameters
3. âœ… **Learning Rate Critical**: lr=1e-3 outperforms 3e-4 by 8% and 1e-4 by 44%
4. âœ… **Cosine Decay Essential**: Enables long-term refinement (83.6% @ epoch 15 â†’ 97.4% @ epoch 48)

## Key Findings

### EDA Insights
1. âœ… **Perfect Class Balance**: All 150 identities have exactly 72 images
2. âœ… **Uniform Dimensions**: Consistent 112Ã—112 resolution simplifies preprocessing
3. âœ… **Data Quality**: No null or corrupted files detected (Spark validation)
4. âœ… **Color Information**: RGB channels show meaningful variance (R: 0.397, G: 0.321, B: 0.279), justifying color retention over grayscale

### Feature Selection Rationale
- **Color vs Grayscale**: Channel variance analysis showed RGB captures more discriminative features
- **Resolution**: Native 112Ã—112 retained to avoid interpolation artifacts
- **Augmentation**: Aggressive augmentation compensates for limited dataset size and improves robustness

### Model Design Decisions
- **Residual Connections**: Enable deeper networks and better gradient flow through skip connections
- **Separable Convolutions**: Reduce parameters by ~8-9Ã— while maintaining representational power
- **Batch Normalization**: Stabilize training and enable higher learning rates (1e-3)
- **Dropout (0.25)**: Optimal regularization preventing overfitting while enabling 100% training accuracy
- **Cosine Learning Rate Decay**: Enables continued refinement in later epochs (1e-3 â†’ 1e-5)
- **Global Average Pooling**: Reduces parameters and prevents spatial overfitting compared to flatten+dense

## Hyperparameter Tuning

### Manual Variant Sweep (15 epochs each)
| Variant | Dropout | Learning Rate | Val Acc @ Epoch 15 | Train-Val Gap |
|---------|---------|---------------|-------------------|---------------|
| **Original Baseline** | 0.25 | 1e-3 (cosine) | **83.62%** | 13.62% |
| Variant: Baseline | 0.25 | 3e-4 (constant) | 81.00% | 12.17% |
| Variant: Regularised | 0.35 | 1e-4 (constant) | 44.90% | 1.98% (underfit) |
| **KerasTuner Best** | 0.25 | 1e-3 (constant) | **89.49%** â­ | 7.06% |

**Key Insights**:
- Learning rate is the most critical hyperparameter (lr=1e-3 outperforms 3e-4 by ~8%, and 1e-4 by ~44%)
- Dropout=0.25 is optimal (higher values like 0.35 severely underperform when combined with low LR)
- Constant lr=1e-3 achieves best short-term performance (89.49% @ 15 epochs)
- Cosine decay lr=1e-3â†’1e-5 achieves best long-term performance (97.43% @ 48 epochs)

### KerasTuner Results
- **Search Strategy**: Hyperband (30 trials, max 10 epochs per trial)
- **Total Runtime**: 53 minutes 28 seconds
- **Best Configuration Discovered**: 
  - **Dropout: 0.25** (exact match to original model)
  - **Learning Rate: 1e-3** (exact match to original model)
- **Best Validation Accuracy**: 82.85% (at 10 epochs)
- **Validation**: KerasTuner independently confirmed the original hyperparameters as optimal through automated search across 28 configurations

### Comparative Analysis
Three independent experiments converged on the same hyperparameters:
1. **Original Model** (manual choice): dropout=0.25, lr=1e-3 â†’ **97.43% validation**
2. **Variant Sweep** (manual testing): Confirmed lr=1e-3 >> 3e-4 >> 1e-4
3. **KerasTuner** (automated search): Independently discovered dropout=0.25, lr=1e-3

This triangulation provides strong empirical validation of the hyperparameter choices.

## Project Structure

```
PDAN8412_POE/
â”œâ”€â”€ PDAN_POE.ipynb          # Main analysis notebook
â”œâ”€â”€ README.md               # This file
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ raw/                # Original DigiFace-1M images
â”‚   â””â”€â”€ organised/          # Spark-processed train/val/test splits
â”œâ”€â”€ save_at_{epoch}.keras   # Model checkpoints
â””â”€â”€ kt_logs/                # KerasTuner experiment logs
```

## Reproducibility

### Requirements
```
tensorflow>=2.15
keras>=3.0
pyspark>=3.5
matplotlib>=3.7
scikit-learn>=1.3
keras-tuner>=1.4
numpy>=1.24
pillow>=10.0
```

### Running the Analysis
1. **Mount Google Drive** (if using Colab):
   ```python
   from google.colab import drive
   drive.mount('/content/drive')
   ```

2. **Extract Dataset**:
   ```bash
   unzip facial_recognition_dataset.zip -d ./dataset/raw
   ```

3. **Execute Notebook Cells Sequentially**:
   - Section 1: Introduction & Dataset Overview
   - Section 2: Spark EDA
   - Section 3: Feature Engineering & Preprocessing
   - Section 4: Model Training
   - Section 5: Evaluation & Tuning
   - Section 6: Results & Reporting

4. **Toggle Experimental Flags**:
   - `RUN_VARIANT_SWEEP = True` for manual hyperparameter search
   - `RUN_TUNER = True` for KerasTuner optimization
   - `RUN_FINAL_TRAIN = True` for final model retraining

## Evaluation & Diagnostics

### Implemented Evaluations
1. âœ… **Training History Plots**: Accuracy and loss curves over 50 epochs
2. âœ… **Classification Report**: Per-class precision, recall, F1-score (macro avg: 97.3%)
3. âœ… **Confusion Matrix**: 150Ã—150 heatmap showing minimal off-diagonal confusion
4. âœ… **Sample Predictions**: Visual inspection of model outputs (100% accuracy on 6 samples)
5. âœ… **Hyperparameter Sweeps**: Manual variant sweep (3 configs) + KerasTuner (30 trials)
6. âœ… **Comparative Analysis**: 4-model comparison with training curves, bar charts, and efficiency metrics
7. âœ… **Learning Efficiency Analysis**: Epochs required to reach validation accuracy milestones

### Addressing Rubric Requirements
- **Thorough EDA**: Spark-based analysis with visualizations
- **Feature Selection Justification**: Color channel variance analysis
- **Model Retraining**: Multiple configurations tested (manual + KerasTuner)
- **Comprehensive Evaluation**: Multiple metrics and visualization techniques
- **Repeatable Experiments**: Seeded random splits and toggleable experiment flags

## Limitations & Future Work

### Current Limitations
1. **Dataset Size**: 10,800 images is sufficient for proof-of-concept but production systems typically use 100k+ images
2. **Synthetic Data**: DigiFace-1M is photorealistic but may not fully capture real-world variations (aging, extreme lighting, motion blur)
3. **Compute Constraints**: Training limited to Google Colab resources (15-16 minutes for 50 epochs is acceptable but could be faster with dedicated GPUs)
4. **Small Test Sample Sizes**: Some classes have only 2-5 test samples, making per-class metrics more variable
5. **Static Identity Set**: Model trained on fixed 150 identities; cannot recognize new identities without retraining

### Proposed Improvements
1. **Dataset Expansion**:
   - Generate additional DigiFace-1M identities
   - Augment with VGGFace2-lite or similar open datasets
   - Target 50k+ images for improved robustness

2. **Model Enhancements**:
   - Explore deeper architectures (ResNet50, EfficientNet)
   - Implement triplet loss for metric learning
   - Test transfer learning from pre-trained face recognition models

3. **Production Readiness**:
   - Add confidence thresholding for unknown faces
   - Implement real-time inference optimization
   - Deploy with Flask/FastAPI REST API

## References

1. **DigiFace-1M Dataset**: [Microsoft Research](https://github.com/microsoft/DigiFace1M)
2. **TensorFlow Documentation**: [tensorflow.org](https://www.tensorflow.org/)
3. **Keras API Reference**: [keras.io](https://keras.io/)
4. **Apache Spark**: [spark.apache.org](https://spark.apache.org/)
5. **KerasTuner**: [keras.io/keras_tuner](https://keras.io/keras_tuner/)
6. **scikit-learn**: [scikit-learn.org](https://scikit-learn.org/)

## Author & Context

**Author** David Roy Mellors, ST10241466
**Course**: PDAN8412 - Portfolio of Evidence  
**Institution**: The IIE's Varisty College  
**Date**: November 2025  
**Purpose**: Academic assessment demonstrating deep learning and big data processing skills

## License

This project is submitted for academic assessment under non-commercial research use terms. The DigiFace-1M dataset is licensed for non-commercial research purposes only.

---

## Summary

This facial recognition system demonstrates **production-ready performance** with 97.6% test accuracy on a challenging 150-class problem. The model successfully handles:
- âœ… Heavy occlusions (masks covering 40%+ of face)
- âœ… Accessories (sunglasses, hats, head coverings)
- âœ… Lighting variations (indoor, outdoor, directional)
- âœ… Pose diversity (frontal, profile, angled views)
- âœ… Ethnic diversity (wide range of skin tones and facial structures)

**Key Technical Contributions**:
- Efficient CNN architecture using separable convolutions and residual connections
- Empirically validated hyperparameters through triangulation (manual, automated, and original)
- Comprehensive evaluation with 7 different diagnostic techniques
- Spark-powered EDA demonstrating big data processing capabilities
- Reproducible training pipeline with toggleable experiment flags

**Note**: This project demonstrates proficiency in:
- Large-scale data processing with Apache Spark (10,800 images)
- Deep learning model development with TensorFlow/Keras (40-layer CNN)
- Systematic hyperparameter optimization (manual + KerasTuner with 30 trials)
- Comprehensive model evaluation and reporting (7 evaluation techniques)
- Reproducible research practices (seeded splits, toggleable flags, detailed documentation)

