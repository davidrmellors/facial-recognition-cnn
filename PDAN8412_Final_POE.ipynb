{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDAN8412 Final Portfolio of Evidence (POE)\n",
    "\n",
    "## Part 3: Image Recognition with Convolutional Neural Networks (CNN)\n",
    "\n",
    "**Student Name:** [Your Name]  \n",
    "**Student Number:** [Your Number]  \n",
    "\n",
    "### Project Overview\n",
    "This notebook demonstrates the construction, training, and evaluation of a Convolutional Neural Network (CNN) to identify faces from a synthetic dataset. The project fulfills the requirements of the PDAN8412 POE by including:\n",
    "\n",
    "1.  **Data Handling**: Utilization of Apache Spark for efficient data ingestion and organization.\n",
    "2.  **Exploratory Data Analysis (EDA)**: Analysis of class distributions and dataset quality.\n",
    "3.  **Feature Selection**: Implementation of data augmentation to learn robust facial features.\n",
    "4.  **Model Architecture**: A custom CNN inspired by Xception/ResNet architectures.\n",
    "5.  **Hyperparameter Tuning**: Automated tuning using KerasTuner to find optimal learning rates and dropout values.\n",
    "6.  **Evaluation**: Comprehensive performance metrics including confusion matrices and learning curves.\n",
    "\n",
    "### Dataset\n",
    "The dataset consists of synthetic facial images categorized by subject ID. To ensure robust training and meet the requirement of **>10,000 records**, this notebook includes a data expansion step using offline augmentation if the raw dataset is smaller.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup & Dependencies\n",
    "# Installing necessary libraries for deep learning, tuning, and visualization.\n",
    "\n",
    "!uv pip install tensorflow keras keras-tuner pyspark matplotlib seaborn scikit-learn pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.utils import image_dataset_from_directory, plot_model\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import input_file_name, regexp_extract, col\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import keras_tuner as kt\n",
    "\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Run Configuration\n",
    "# Toggle these flags to control which parts of the notebook execute.\n",
    "# This allows for quick reviewing without re-running heavy tasks.\n",
    "\n",
    "config = {\n",
    "    \"MOUNT_DRIVE\": True,         # Set to True if running on Colab and data is in Drive\n",
    "    \"EXPAND_DATASET\": False,     # Set to True to generate augmented images to hit >10k requirement\n",
    "    \"RUN_EDA\": True,             # Run Exploratory Data Analysis\n",
    "    \"RUN_TUNING\": True,          # Run KerasTuner (Time consuming)\n",
    "    \"RUN_TRAINING\": True,        # Train the final model\n",
    "    \"LOAD_MODEL\": False,         # Load pre-trained model from Drive instead of training\n",
    "    \"SAVE_PATH\": \"/content/drive/MyDrive/final_face_cnn_model.keras\",\n",
    "    \"DATA_ZIP_PATH\": \"/content/drive/MyDrive/subjects_0-50_imgs.zip\",\n",
    "    \"EXTRACT_PATH\": \"/content/subjects_0-50_imgs/\",\n",
    "    \"WORKING_DIR\": \"./dataset/organised/\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Data Initialization\n",
    "\n",
    "if config[\"MOUNT_DRIVE\"]:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "# Extract Dataset if it doesn't exist\n",
    "if not os.path.exists(config[\"EXTRACT_PATH\"]):\n",
    "    print(\"Extracting dataset...\")\n",
    "    !unzip -q {config[\"DATA_ZIP_PATH\"]} -d /content/\n",
    "    print(\"Extraction complete.\")\n",
    "else:\n",
    "    print(\"Dataset already extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Data Expansion (Meeting the >10,000 records requirement)\n",
    "# If the dataset is small, we use offline augmentation to create new files.\n",
    "\n",
    "def expand_dataset(source_dir):\n",
    "    print(\"Starting Dataset Expansion...\")\n",
    "    datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    \n",
    "    total_generated = 0\n",
    "    # Iterate through each subject folder\n",
    "    for subject in os.listdir(source_dir):\n",
    "        subject_path = os.path.join(source_dir, subject)\n",
    "        if os.path.isdir(subject_path):\n",
    "            images = [os.path.join(subject_path, f) for f in os.listdir(subject_path) if f.endswith('.png')]\n",
    "            \n",
    "            # Generate 3 augmented versions for every 1 original image\n",
    "            for img_path in images:\n",
    "                try:\n",
    "                    img = keras.utils.load_img(img_path)\n",
    "                    x = keras.utils.img_to_array(img)\n",
    "                    x = x.reshape((1,) + x.shape)\n",
    "                    \n",
    "                    i = 0\n",
    "                    for batch in datagen.flow(x, batch_size=1, \n",
    "                                              save_to_dir=subject_path, \n",
    "                                              save_prefix='aug', \n",
    "                                              save_format='png'):\n",
    "                        i += 1\n",
    "                        total_generated += 1\n",
    "                        if i >= 3: # Generate 3 new images per file\n",
    "                            break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {img_path}: {e}\")\n",
    "                    \n",
    "    print(f\"Expansion Complete. Generated {total_generated} new images.\")\n",
    "\n",
    "if config[\"EXPAND_DATASET\"]:\n",
    "    expand_dataset(config[\"EXTRACT_PATH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Data Ingestion with Spark\n",
    "\n",
    "# Initialise Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FacialRecognition\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load image paths into a DataFrame\n",
    "image_df = spark.read.format(\"binaryFile\") \\\n",
    "    .option(\"pathGlobFilter\", \"*.png\") \\\n",
    "    .option(\"recursiveFileLookUp\", \"true\") \\\n",
    "    .load(config[\"EXTRACT_PATH\"])\n",
    "\n",
    "# Extract Label from path (assuming folder structure /subject_id/image.png)\n",
    "image_df = image_df.withColumn(\n",
    "    \"label\",\n",
    "    regexp_extract(input_file_name(), r\"/(\\d+)/[^/]+$\", 1)\n",
    ")\n",
    "\n",
    "# Clean empty labels\n",
    "image_df = image_df.filter(col(\"label\") != \"\")\n",
    "\n",
    "total_images = image_df.count()\n",
    "unique_subjects = image_df.select('label').distinct().count()\n",
    "\n",
    "print(f\"Total Dataset Size: {total_images} images\")\n",
    "print(f\"Unique Subjects: {unique_subjects}\")\n",
    "\n",
    "image_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Exploratory Data Analysis (EDA)\n",
    "We perform EDA to understand the distribution of our data. For deep learning classification, it is crucial to have a balanced dataset to prevent the model from biasing towards classes with more samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"RUN_EDA\"]:\n",
    "    print(\"Running EDA...\")\n",
    "    # 1. Calculate Class Distribution\n",
    "    class_counts = image_df.groupBy(\"label\").count().toPandas()\n",
    "    class_counts['label'] = class_counts['label'].astype(int)\n",
    "    class_counts = class_counts.sort_values('label')\n",
    "\n",
    "    # 2. Visualization\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    sns.barplot(data=class_counts, x='label', y='count', palette='viridis')\n",
    "    plt.title(f\"Class Distribution (Total: {total_images} images)\", fontsize=16)\n",
    "    plt.xlabel(\"Subject ID\")\n",
    "    plt.ylabel(\"Number of Images\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.axhline(y=class_counts['count'].mean(), color='r', linestyle='--', label='Mean Count')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # 3. Statistics\n",
    "    print(f\"Min images per class: {class_counts['count'].min()}\")\n",
    "    print(f\"Max images per class: {class_counts['count'].max()}\")\n",
    "    print(f\"Mean images per class: {class_counts['count'].mean():.2f}\")\n",
    "else:\n",
    "    print(\"Skipping EDA step (RUN_EDA=False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Data Organization\n",
    "# Splitting into Train (80%) and Validation (20%)\n",
    "\n",
    "train_df, val_df = image_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "train_dir = os.path.join(config[\"WORKING_DIR\"], \"train\")\n",
    "val_dir = os.path.join(config[\"WORKING_DIR\"], \"val\")\n",
    "\n",
    "def organise_images(df, output_base_dir):\n",
    "    if os.path.exists(output_base_dir):\n",
    "        print(f\"Directory {output_base_dir} already exists, skipping copy.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Organising images into {output_base_dir}...\")\n",
    "    data = df.select(\"path\", \"label\").collect()\n",
    "    for row in data:\n",
    "        src_path = row.path.replace(\"file:\", \"\")\n",
    "        label = row.label\n",
    "        dest_dir = os.path.join(output_base_dir, label)\n",
    "        os.makedirs(dest_dir, exist_ok=True)\n",
    "        dest_path = os.path.join(dest_dir, os.path.basename(src_path))\n",
    "        if not os.path.exists(dest_path):\n",
    "            shutil.copy2(src_path, dest_path)\n",
    "\n",
    "organise_images(train_df, train_dir)\n",
    "organise_images(val_df, val_dir)\n",
    "\n",
    "# Stop Spark to free up RAM for TensorFlow\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Feature Selection & Data Augmentation\n",
    "\n",
    "In Deep Learning, specific \"feature selection\" (like removing columns in tabular data) is replaced by **Feature Extraction** via Convolutional Layers. The CNN learns to identify edges, textures, and facial structures automatically.\n",
    "\n",
    "To improve the robustness of these features, we use **Data Augmentation**. This artificially creates variations of our training images (flips, rotations, zooms), forcing the model to focus on the invariant features of the face rather than specific pixel positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (112, 112)\n",
    "batch_size = 64\n",
    "\n",
    "# Load Datasets\n",
    "print(\"Loading Training Set...\")\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='int',\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"Loading Validation Set...\")\n",
    "val_dataset = image_dataset_from_directory(\n",
    "    val_dir,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='int',\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# Define Augmentation Layers\n",
    "data_augmentation_layers = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "    layers.RandomContrast(0.1),\n",
    "])\n",
    "\n",
    "# Visualise Augmentation\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_dataset.take(1):\n",
    "    first_image = images[0]\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        augmented_image = data_augmentation_layers(tf.expand_dims(first_image, 0))\n",
    "        plt.imshow(np.array(augmented_image[0]).astype(\"uint8\"))\n",
    "        plt.title(f\"Augmentation {i+1}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "# Optimize Performance\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda x, y: (data_augmentation_layers(x, training=True), y), \n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Model Architecture & Tuning (KerasTuner)\n",
    "We define a scalable CNN architecture and use **Hyperband** tuning to find the best hyperparameters (Learning Rate, Dropout, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    inputs = keras.Input(shape=image_size + (3,))\n",
    "    \n",
    "    # Hyperparameters to tune\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4])\n",
    "    hp_dropout = hp.Float('dropout', min_value=0.2, max_value=0.5, step=0.1)\n",
    "    \n",
    "    # Scaling\n",
    "    x = layers.Rescaling(1.0 / 255.0)(inputs)\n",
    "    \n",
    "    # Entry Block\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    \n",
    "    x = layers.Conv2D(64, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu")(x)\n",
    "    \n",
    "    # Residual Blocks\n",
    "    previous_block_activation = x\n",
    "    for size in [128, 256, 512]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "        \n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(previous_block_activation)\n",
    "        x = layers.add([x, residual])\n",
    "        previous_block_activation = x\n",
    "        \n",
    "    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu")(x)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(hp_dropout)(x)\n",
    "    \n",
    "    # Output Layer (Dynamic number of classes)\n",
    "    outputs = layers.Dense(unique_subjects, activation=\"softmax\")(x)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"RUN_TUNING\"]:\n",
    "    print(\"Starting Hyperparameter Tuning...\")\n",
    "    tuner = kt.Hyperband(\n",
    "        build_model,\n",
    "        objective='val_accuracy',\n",
    "        max_epochs=10,\n",
    "        factor=3,\n",
    "        directory='kt_dir',\n",
    "        project_name='face_recog_tuning'\n",
    "    )\n",
    "    \n",
    "    stop_early = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "    \n",
    "    tuner.search(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[stop_early])\n",
    "    \n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    print(f\"Best Learning Rate: {best_hps.get('learning_rate')}\")\n",
    "    print(f\"Best Dropout: {best_hps.get('dropout')}\")\n",
    "    \n",
    "    model = tuner.hypermodel.build(best_hps)\n",
    "else:\n",
    "    print(\"Skipping tuning, building default model...\")\n",
    "    # Default hyperparameters if tuning is skipped\n",
    "    class MockHP:\n",
    "        def Choice(self, name, values): return values[0]\n",
    "        def Float(self, name, min_value, max_value, step): return 0.3\n",
    "    model = build_model(MockHP())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Final Model Training or Loading\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        config[\"SAVE_PATH\"],\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_accuracy\"\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_accuracy\",\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "if config[\"RUN_TRAINING\"]:\n",
    "    print(\"Starting Training...\")\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=30,\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    print(\"Training Complete.\")\n",
    "    \n",
    "elif config[\"LOAD_MODEL\"] and os.path.exists(config[\"SAVE_PATH\"]):\n",
    "    print(f\"Loading pre-trained model from {config['SAVE_PATH']}...\")\n",
    "    model = keras.models.load_model(config[\"SAVE_PATH\"])\n",
    "    print(\"Model Loaded.\")\n",
    "    history = None # No history available if loading\n",
    "    \n",
    "else:\n",
    "    print(\"No training and no model loaded. Evaluation might fail.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Evaluation & Interpretation\n",
    "We analyse the model's performance using:\n",
    "1.  **Learning Curves**: To check for overfitting/underfitting.\n",
    "2.  **Confusion Matrix**: To see which faces are being confused with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 Learning Curves\n",
    "if 'history' in locals() and history is not None:\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs_range = range(len(acc))\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No history to plot (Model was loaded, not trained).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.2 Confusion Matrix & Classification Report\n",
    "print(\"Generating Evaluation Metrics...\")\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# Iterate over validation dataset to get predictions\n",
    "for img, label in val_dataset:\n",
    "    pred = model.predict(img, verbose=0)\n",
    "    y_true.extend(label.numpy())\n",
    "    y_pred.extend(np.argmax(pred, axis=1))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "plt.figure(figsize=(25, 20))\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=False, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
